<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title> WildLive</title>
  <style>
    body { font-family: Georgia, sans-serif; background-color: #f4f4f4; margin: 0; padding: 0; }
    header {
      background: linear-gradient(135deg, #63b1f1 0%, #0bf532 100%);
      color: #fff;
      text-align: center;
      padding: 80px 20px;
      box-shadow: inset 0 0 50px rgba(0, 0, 0, 0.2);
      position: relative;
      overflow: hidden;
    }
    header::before {
      content: "";
      position: absolute;
      top: 0; left: 0; right: 0; bottom: 0;
      background: url('https://www.transparenttextures.com/patterns/asfalt-light.png') repeat;
      opacity: 0.15;
      pointer-events: none;
      z-index: 1;
    }
    header h1 {
      font-size: 3.5em;
      font-weight: 700;
      margin: 0 0 20px 0;
      text-shadow: 0 2px 5px rgba(0,0,0,0.3);
      position: relative;
      z-index: 2;
      line-height: 1.2;
    }  
    header p {
      font-size: 1.0em;
      font-weight: 500;
      margin: 0 0 30px 0;
      position: relative;
      z-index: 2;
      line-height: 1.2;
    }  
    header p a:hover {
      color: #e0ffdd;
    }
    .section.links p {
      margin: 0;
      position: relative;
      z-index: 2;
    }
    .section.links a {
      background: #006622;
      padding: 10px 18px;
      margin: 0 8px;
      border-radius: 25px;
      color: #fff;
      font-weight: 600;
      text-decoration: none;
      transition: background 0.3s ease;
      display: inline-block;
    }
    .section.links a:hover {
      background: #090df1;
    }
    .group-top a {
      font-size: 1.1em;           /* Bigger font size */
      color: #ffffff;             /* Bright blue color */
      font-weight:600;           /* Semi-bold */
      text-decoration: underline;      /* Remove underline */
      margin-right: 12px;
    }
    .group-top a:hover {
      color: #090df1;
      text-decoration: underline; /* Underline on hover */
    }

    .group-bottom a {
      font-size: 1.0em;             /* Smaller font size */
      color: #ffffff;             /* Grayish color */
      font-weight: 500;           /* Normal weight */
      text-decoration: none; /* Keep underline */
      margin-right: 10px;
    }
    .group-bottom a:hover {
      text-decoration: underline;
      color: #090df1;             /* Change color on hover */
    }    
    .content { padding: 20px; max-width: 1500px; margin: 0 auto; background: white; border-radius: 8px; box-shadow: 0 0 15px rgba(0, 0, 0, 0.1); }
    .section { margin-bottom: 30px;margin-top: 30px }
    h2 {  color: #178501; font-size: 2.5em; }
    p { font-size:1.4em; line-height: 1.6;  color: #555454;}
    /* p2 { font-size: 15px; line-height: 1.6; color: #333; } */
    .abstract { font-size: 1.4em; font-style: italic; color: #555454; text-align: justify;}
    .links a { padding: 10px 20px; background: #1b17d8; color: white; text-decoration: none; border-radius: 4px; }
    .links a:hover { background: #2980b9; }
    footer { text-align: center; padding: 20px; background-color: #ecf0f1; font-size: 14px; }
    .image-container {
      display: flex;
      /* justify-content:space-between; */
      justify-content:center;
      align-items: center;
      gap: 10px;
      flex-wrap: wrap; /* Allow images to wrap on smaller screens */
    }

    .image-container img {
      height: 80vh;
      width: auto;
      object-fit: cover;
      margin: 0 50px
    }
    .image-container2 {
      display: flex;
      justify-content: space-around;
      align-items: center;
      gap: 20px;
      flex-wrap: wrap;
      margin-top: 40px;
    }

    .image-container2 img {
      max-height: 450px;
      width: auto;
      object-fit: contain;
      margin: 0 20px;
    }

    /* Media query for small screens (mobile) */
    @media (max-width: 768px) {
      .image-container {
        flex-direction: column; /* Stack images vertically on small screens */
        align-items: center; /* Center the images */
      }

      .image-container img {
        height: 50vh; 
        
      }
      .image-container2 {
        flex-direction: column;
        align-items: center;
      }

      .image-container2 img {
        max-height: 200px;
      }
    }
    .side-by-side {
    display: flex;
    justify-content: center;
    align-items: flex-start;
    gap: 40px;
    margin-top: 40px;
    margin-bottom: 100px;
    flex-wrap: wrap; /* Allows wrapping on smaller screens */
  }

  /* Left: Image section */
  .side-by-side .image-box {
    flex: 0 0 75%; /* Give 60% width to the image */
    text-align: center;
  }

  /* Image size */
  .side-by-side .image-box img {
    height: 110%; /* Image height */
    width: 100%;  /* Full width inside the container */
    object-fit: contain;
    display: inline-block;
  }

  /* Right: Tree section */
  .side-by-side .tree {
    flex: 0 0 20%; /* Give 35% width to the tree (less than the image) */
    font-family: monospace;
    white-space: pre;
    font-size: 14px;
  }
  .results-flex {
  display: flex;
  align-items: flex-start;
  gap: 30px;
  margin-top: 20px;
  flex-wrap: wrap; /* Responsive layout */
  }

  .results-text {
    flex: 1 1 50%;
  }

  .results-image {
    flex: 1 1 40%;
    text-align: center;
  }

  .results-image img {
    max-width: 100%;
    height: auto;
    border-radius: 8px;
  }

  /* Responsive fallback */
  @media (max-width: 768px) {
    .results-flex {
      flex-direction: column;
      align-items: center;
    }

    .results-text, .results-image {
      flex: 1 1 100%;
      text-align: center;
    }
  }

  /* Media query for small screens */
  @media (max-width: 768px) {
      header h1 {
        font-size: 2.2rem;
      }
      header p {
        font-size: 1rem;
      }
      .section.links a {
        margin: 6px 4px;
        padding: 8px 14px;
      }
    }
  @media (max-width: 768px) {
    .side-by-side {
      flex-direction: column;
      align-items: center;
    }

    /* Adjust the image size on smaller screens */
    .side-by-side .image-box img {
      width: 80%;
      height: auto;
    }

    /* Adjust tree structure width for small screens */
    .side-by-side .tree {
      width: 100%;
    }
  }
  </style>
</head>

<body>
  <!-- <div style="position: relative; width: 100%; padding-top: 30%; overflow: hidden; margin: 0;">
    <img src="title.png" alt="Cropped image"
         style="position: absolute; top: -20%; left: 0; width: 100%; height: 80%; display: block;">
  </div> -->
  
  <!-- Header Section -->
  <header>
    <h1>WildLive: Near Real-time Visual Wildlife Tracking onboard UAVs</h1>
    <p class="group-top">
      <a href="https://research-information.bris.ac.uk/en/persons/dat-nguyen-ngoc" target="_blank">Nguyen Ngoc Dat</a>, 
      <a href="https://www.bristol.ac.uk/people/person/Tom-Richardson-63e47259-1d08-4e30-9353-9b1b22e0f749/" target="_blank">Tom Richardson</a>, 
      <a href="https://www.bristol.ac.uk/people/person/Matthew-Watson-41f9c67d-43b5-454d-9bd2-495bd9dd6ba6/" target="_blank">Matthew Watson</a>, 
      <a href="https://research-information.bris.ac.uk/en/persons/kilian-meier" target="_blank">Kilian Meier</a>, 
      <a href="https://scholar.google.com/citations?user=bhNDuigAAAAJ&hl=en" target="_blank">Jenna Marie Kline</a>, 
      <a href="https://research-information.bris.ac.uk/en/persons/sid-reid" target="_blank">Sid Reid</a>, 
      <a href="https://portal.findresearcher.sdu.dk/en/persons/guym" target="_blank">Guy Maalouf</a>, 
      <a href="https://www.linkedin.com/in/duncan-hine-uas-usv/?originalSubdomain=uk" target="_blank">Duncan Hine</a>, 
      <a href="https://www.bristol.ac.uk/people/person/Majid-Mirmehdi-93fe13c3-06b3-4797-a8fe-9fd6d2dc5dc2/" target="_blank">Majid Mirmehdi</a>, 
      <a href="https://www.bristol.ac.uk/people/person/Tilo-Burghardt-70f598b4-d736-4ebe-b1e4-d0e8ba97aa2b/" target="_blank">Tilo Burghardt</a>
    </p>

    <p class="group-bottom">
      <a href="https://www.bristol.ac.uk/" target="_blank">University of Bristol, UK</a>, 
      <a href="https://www.osu.edu/" target="_blank">Ohio State University, USA</a>, 
      <a href="https://www.sdu.dk/en" target="_blank">University of Southern Denmark, Denmark</a>
    </p>

    <div class="section links">
      <p>
        <a href="https://github.com/dat-nguyenvn/DC12" target="_blank">Github (Coming soon)</a> |
        <a href="https://your-link-to-bibtex.com/bibtex.bib" target="_blank">Dataset (Coming soon)</a> |
        <a href="https://arxiv.org/abs/2504.10165" target="_blank">Arxiv</a> 
        <!-- <a href="wildlive.pdf" download>Download PDF</a> -->
      </p>
    </div>
  </header>

<!-- 
  <div style="width: 100%; height: 45vh; text-align: center;margin-top: 40px;margin-bottom: 80px;">
    <img src="zoomin.png" alt="Description of the image"
         style="height:110%; width: 60%; display: inline-block;">

  </div> -->
<div style="width: 100%; text-align: center; margin-top: 40px;">
  <video autoplay muted loop playsinline style="width: 50%; border-radius: 12px; box-shadow: 0 4px 12px rgba(0,0,0,0.2);">
    <source src="demo3_com.mp4" type="video/mp4">
    Your browser does not support the video tag.
  </video>
  <p style="margin-top: 20px; font-size: 1.2em; color: #555454;">
    WildLive result
  </p>
</div>

  <!-- Main Content Section -->
  <div class="content">
    <!-- Abstract Section -->
    <div class="section">
      <h2>Abstract</h2>
      <p class="abstract"> <b>Live tracking of wildlife via high-resolution video processing directly onboard drones is widely unexplored and most existing solutions 
        rely on streaming video to ground stations to support navigation. Yet, both autonomous animal-reactive flight control beyond visual line of sight and/or
        mission-specific individual and behaviour recognition tasks rely to some degree on this capability. In response, we introduce WildLive – a near real-time animal detection and
        tracking framework for high-resolution imagery running directly onboard uncrewed aerial vehicles (UAVs). The
        system performs multi-animal detection and tracking at 17.81 fps for HD and 7.53 fps on 4K video streams suitable for operation during higher altitude flights to minimise
        animal disturbance. Our system is optimised for Jetson Orin AGX onboard hardware. It integrates the efficiency of
        sparse optical flow tracking and mission-specific sampling with device-optimised and proven YOLO-driven object 
        detection and segmentation techniques. Essentially, computational resource is focused onto spatio-temporal regions
        of high uncertainty to significantly improve UAV processing speeds. Alongside, we introduce our WildLive dataset,
        which comprises 200K+ annotated animal instances across 19K+ frames from 4K UAV videos collected at the Ol Pejeta Conservancy in Kenya. 
        All frames contain ground truth bounding boxes, segmentation masks, as well as individual tracklets and tracking point trajectories. We compare
        our system against current object tracking approaches including OC-SORT, ByteTrack, and SORT. Our multi-animal
        tracking experiments with onboard hardware confirm that near real-time high-resolution wildlife tracking is possible
        on UAVs whilst maintaining high accuracy levels as needed for future navigational and mission-specific animal-centric
        operational autonomy.</b></p>
    </div>

    <!-- Introduction Section -->
    <div class="section">
      <h2>The WildLive Benchmark Dataset (will be available soon)</h2>
      <p style="text-align: justify;">Our dataset contains 215,800 bounding boxes and animal segmentation masks along its 291 zebra, giraffe,
         and elephant tracklets, plus 84 point tracks across 22 UAV-acquired 4K video sequences,
          totaling 19,139 frames recorded on site at the Ol Pejeta Conservancy in Kenya. 
          Acquisition was conducted via DJI Mavic 3 Enterprise and Pro drones plus a custom-built quadcopter for wildlife missions.
          Overall, the dataset provides verified animal bounding boxes and SAM2 segmentations,
           tracklet IDs, as well as manually corrected, sparse LK pixel trajectories. </p>

    

      <!-- Image section -->

      <div class="side-by-side">
        <!-- Left: Image -->
        <div class="image-box">
          <img src="merge2.png" alt="Description of the image">
          <p style="margin-top: 20px; font-size: 1.2em; color: #555454;">
            Dataset overview
          </p>
        </div>
      
        <!-- Right: Tree -->
        <div class="tree">

      data/
      ├── video1/
      │   ├── frames/
      │   │   ├── frame_0.jpg
      │   │   └── frame_1.jpg
      │   ├── masksam/
      │   │   ├── frame_0.json
      │   │   └── frame_1.json
      │   ├── boxid/
      │   │   ├── frame_0.json
      │   │   └── frame_1.json
      │   └── point/
      │       ├── frame_0.json
      │       └── frame_1.json
      ├── video2/
      │   ├── frames/
      │   ├── masksam/
      │   ├── boxid/
      │   └── point/
        </div>
      </div>



    <!-- <div class="image-container">
      <div style="width: 100%; height: 45vh; text-align: center;margin-top: 40px;margin-bottom: 100px;">
        <img src="merge2.png" alt="Description of the image"
            style="height:110%; width: 60%; display: inline-block;">
      </div>

      <div class="tree">
        data/
        ├──video1/
            ├── frames/
                ├──  frame_0.jpg
                ├──  frame_1.jpg
            ├── masksam/
                ├──  frame_0.json
                ├──  frame_1.json
            ├── boxid/
                ├──  frame_0.json
                ├──  frame_1.json
            └── point/
                ├──  frame_0.json
                ├──  frame_1.json

        ├── video2/
            ├── frames/
            ├── masksam/
            ├── boxid/
            └── point/      
      </div>
    </div> -->
    <!-- Method Section -->
    <div style="width: 100%; text-align: center; margin-top: 40px;">
      <video autoplay muted loop playsinline style="width: 80%; border-radius: 12px; box-shadow: 0 4px 12px rgba(0,0,0,0.2);">
        <source src="sample_annotation.mp4" type="video/mp4">

      </video>
      <p style="margin-top: 20px; font-size: 1.2em; color: #555454;">
        WildLive annotation sample: includes boxes, ID, masks and subset point trajectories.
      </p>
    </div>

    <div class="section">
      <h2>WildLive Method.</h2>
      <p style="text-align: justify;">
        
          Our WildLive framework is optimised for near real-time high-resolution video stream tracking onboard drones.
          It integrates SAHI sampling and YOLO object detection with light-weight sparse optical flow LK tracking and 
          YOLO instance segmentation to localise and follow animals live on UAV platforms. We compare our system against 
          current object tracking approaches including OC-SORT, ByteTrack, and SORT. Our multi-animal tracking experiments 
          with onboard hardware confirm that near real-time high-resolution wildlife tracking is possible on UAVs whilst 
          maintaining high accuracy levels as needed for future navigational and mission-specific animal-centric 
          operational autonomy.
        
      </p>
    </div>
    <div class="image-container">
      <img src="title.png" alt="Image 2">
      <img src="result.png" alt="Image 1">
    </div>
    <!-- Results Section -->

    <div class="section">
      <h2>WildLive Results.</h2>
      <p style="text-align: justify;">
        
        Full results are shown in the above table and confirm order-of-magnitude gains in processing speed (up to 6.32fps) 
        compared to other tested techniques resting on only intermittent re-detection bridged by computationally negligible LKP point tracking.
        Accuracy, maybe surprisingly, slightly improves too, leading to best MOTA at 81.17% and IDF1 at 79.03%:
        utilising PoA and IoU together for ambiguity resolution proves superior compared to IoU-centred full object tracking methods
        tested which have no direct access to segmentation masks.
      </p>
    </div>



    <div class="image-container2">
      <img src="hdexp.png" alt="Image 2">
      <img src="speed.png" alt="Image 1">
    </div>
    <div style="width: 100%; text-align: center; margin-top: 40px;">
      <video autoplay muted loop playsinline style="width: 80%; border-radius: 12px; box-shadow: 0 4px 12px rgba(0,0,0,0.2);">
        <source src="demo1_com.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
      <p style="margin-top: 20px; font-size: 1.2em; color: #555454;">
        WildLive output sample 1
      </p>
    </div>
    <div style="width: 100%; text-align: center; margin-top: 40px;">
      <video autoplay muted loop playsinline style="width: 80%; border-radius: 12px; box-shadow: 0 4px 12px rgba(0,0,0,0.2);">
        <source src="demo2_com.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
      <p style="margin-top: 20px; font-size: 1.2em; color: #555454;">
        WildLive output sample 2
      </p>
    </div>
    <!-- <div style="width: 100%; text-align: center; margin-top: 40px;">
      <video autoplay muted loop playsinline style="width: 80%; border-radius: 12px; box-shadow: 0 4px 12px rgba(0,0,0,0.2);">
        <source src="demo3_com.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
      <p style="margin-top: 20px; font-size: 1.2em; color: #555454;">
        WildLive output sample 3
      </p>
    </div> -->
    <div class="section">
      <h2>Acknowledgements</h2>
      <p style="text-align: justify;">
        This work was supported by the WildDrone project (under the Marie Skłodowska-Curie Actions (MSCA) - grantagreement ID: 101071224) and UK Research and Innova-
        tion (EPSRC/UKRI - project reference: EP/X029077/1),the Imageomics Institute (NSF HDR Award 2118240), and ICICLE (NSF grant OAC-2112606).
        
      </p>
    </div>


    <div class="section">
      <h2>Citation</h2>
      <pre style="background-color: #f8f8f8; padding: 1em; border-radius: 8px; overflow-x: auto; font-family: monospace; font-size: 1em;">
    @inproceedings{
      dat2025wildlive,
      title={WildLive: Near Real-time Visual Wildlife Tracking onboard {UAV}s},
      author={Nguyen Ngoc Dat and Thomas Stuart Richardson and Matthew Watson and Kilian Meier and Jenna Marie Kline and Sid Reid and Guy Maalouf and Duncan Hine and Majid Mirmehdi and Tilo Burghardt},
      booktitle={5th Workshop on CV4Animals: Computer Vision for Animal Behavior Tracking and Modeling, In conjunction with Computer Vision and Pattern Recognition 2025},
      year={2025},
      url={https://openreview.net/forum?id=3P5Av4siHg}
      }
      </pre>
    </div>


  </div>

  <!-- Footer Section -->
  <footer>
    <p>© Maintained by Nguyen Ngoc Dat</p>
  </footer>

</body>
</html>
